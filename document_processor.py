# document_processor.py

import google.generativeai as genai
import json
import time

# í—¬í¼ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
def extract_json_from_response(text):
    try:
        start = text.find('```json') + len('```json')
        end = text.rfind('```')
        if start > -1 and end > -1:
            json_text = text[start:end].strip()
            return json.loads(json_text)
        else:
            return json.loads(text)
    except (json.JSONDecodeError, IndexError) as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\nì›ë³¸ í…ìŠ¤íŠ¸: {text}")
        return None

# í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì´ì „ê³¼ ë™ì¼)
def chunk_text(text, chunk_size=100000, overlap=10000):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end > len(text):
            end = len(text)
        chunks.append({"text": text[start:end], "global_start": start})
        start += chunk_size - overlap
        if start >= len(text):
            break
    return chunks

# --- ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ---
def run_pipeline(document_text, api_key, status_container):
    
    model_name = 'gemini-2.5-flash-lite'
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)

    # 1. Global Summary ìƒì„±
    status_container.write(f"1/3: **{model_name}** ëª¨ë¸ë¡œ **'ì „ì—­ ìš”ì•½'**ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    preamble = document_text[:3000]
    prompt_global_summary = f"""Analyze the preamble of the Thai legal document provided below. Summarize the document's purpose, background, and core principles in 2-3 sentences in Korean. [Preamble text]\n{preamble}"""
    response_summary = model.generate_content(prompt_global_summary)
    generated_global_summary = response_summary.text.strip()

    # 2. ì²­í¬ ë¶„í• 
    status_container.write(f"2/3: ë¬¸ì„œê°€ í¬ë¯€ë¡œ ì‘ì€ ì¡°ê°(ì²­í¬)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    document_chunks = chunk_text(document_text)
    status_container.write(f"ì´ {len(document_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì²­í¬ë³„ë¡œ êµ¬ì¡° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    time.sleep(1)

    # 3. ê° ì²­í¬ë³„ êµ¬ì¡° ë¶„ì„ ë° ê²°ê³¼ ë³‘í•©
    all_headers = []
    prompt_structure_index_only = """The following text is part of a Thai legal document. Identify all hierarchical headers such as 'à¸«à¸¡à¸§à¸”', 'à¸ªà¹ˆà¸§à¸™', and 'à¸¡à¸²à¸•à¸£à¸²'. For each identified element, extract its type, title, and its character start/end positions within the provided text. Return the result as a JSON array. Ensure every object in the array contains 'type', 'title', 'start_index', and 'end_index' keys. Example format: [{"type": "chapter", "title": "à¸«à¸¡à¸§à¸” 1", "start_index": 10, "end_index": 500}] [Text Chunk]\n{text_chunk}"""

    for i, chunk in enumerate(document_chunks):
        status_container.write(f"3/{len(document_chunks)+2}: ì²­í¬ {i+1}/{len(document_chunks)}ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        
        response = model.generate_content(prompt_structure_index_only.format(text_chunk=chunk["text"]))
        headers_in_chunk = extract_json_from_response(response.text)
        
        if headers_in_chunk and isinstance(headers_in_chunk, list):
            for header in headers_in_chunk:
                # ğŸ‘‡ ë°©ì–´ì  ì½”ë“œ: LLM ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ê³  í•„ìˆ˜ í‚¤ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
                if isinstance(header, dict) and all(k in header for k in ['type', 'title', 'start_index', 'end_index']):
                    # ë¡œì»¬ ì¸ë±ìŠ¤ë¥¼ ì „ì—­ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    header["global_start"] = header["start_index"] + chunk["global_start"]
                    header["global_end"] = header["end_index"] + chunk["global_start"]
                    all_headers.append(header)
                else:
                    # í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°, ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
                    status_container.warning(f"ì²­í¬ {i+1}ì—ì„œ ì˜ˆìƒê³¼ ë‹¤ë¥¸ í˜•ì‹ì˜ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {header}")

    # 4. ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    status_container.write("ëª¨ë“  ì²­í¬ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤...")
    unique_headers = list({(h['global_start'], h['title']): h for h in all_headers}.values())
    sorted_headers = sorted(unique_headers, key=lambda x: x['global_start'])

    # 5. ìµœì¢… ê²°ê³¼ ì¡°ë¦½
    for header in sorted_headers:
        header["text"] = document_text[header["global_start"]:header["global_end"]]

    final_json = {
      "global_summary": generated_global_summary,
      "document_title": f"ë¶„ì„ëœ ë¬¸ì„œ (ëª¨ë¸: {model_name})",
      "chapters": sorted_headers
    }
    
    return final_json