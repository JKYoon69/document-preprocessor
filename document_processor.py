# document_processor.py

import google.generativeai as genai
import json
import time # ê° ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€

# í—¬í¼ í•¨ìˆ˜: LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ì´ì „ê³¼ ë™ì¼)
def extract_json_from_response(text):
    try:
        start = text.find('```json') + len('```json')
        end = text.rfind('```')
        if start > -1 and end > -1:
            json_text = text[start:end].strip()
            return json.loads(json_text)
        else:
            return json.loads(text)
    except (json.JSONDecodeError, IndexError) as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\nì›ë³¸ í…ìŠ¤íŠ¸: {text}")
        return None

# â­ï¸ ìƒˆë¡œìš´ í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
def chunk_text(text, chunk_size=100000, overlap=10000):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        # ì²­í¬ì˜ ëì´ ë¬¸ì„œì˜ ëì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •
        if end > len(text):
            end = len(text)
        
        chunks.append({
            "text": text[start:end],
            "global_start": start
        })
        
        # ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ overlapì„ ê³ ë ¤í•˜ì—¬ ì„¤ì •
        start += chunk_size - overlap
        if start >= len(text):
            break
            
    return chunks

# --- ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ (ëŒ€ëŒ€ì ìœ¼ë¡œ ìˆ˜ì •ë¨) ---
def run_pipeline(document_text, api_key, status_container):
    
    model_name = 'gemini-2.5-flash-lite'
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)

    # 1. Global Summary ìƒì„± (ì´ì „ê³¼ ë™ì¼, ì†ë„ ë¹ ë¦„)
    status_container.write(f"1/3: **{model_name}** ëª¨ë¸ë¡œ **'ì „ì—­ ìš”ì•½'**ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    preamble = document_text[:3000]
    prompt_global_summary = f"""Analyze the preamble of the Thai legal document provided below. Summarize the document's purpose, background, and core principles in 2-3 sentences in Korean. [Preamble text]\n{preamble}"""
    response_summary = model.generate_content(prompt_global_summary)
    generated_global_summary = response_summary.text.strip()

    # 2. â­ï¸ ì²­í¬ ë¶„í• 
    status_container.write(f"2/3: ë¬¸ì„œê°€ í¬ë¯€ë¡œ ì‘ì€ ì¡°ê°(ì²­í¬)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    document_chunks = chunk_text(document_text)
    status_container.write(f"ì´ {len(document_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì²­í¬ë³„ë¡œ êµ¬ì¡° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    time.sleep(1) # ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì½ì„ ì‹œê°„ì„ ì¤ë‹ˆë‹¤.

    # 3. â­ï¸ ê° ì²­í¬ë³„ êµ¬ì¡° ë¶„ì„ ë° ê²°ê³¼ ë³‘í•©
    all_headers = []
    
    # ğŸ‘‡ ìƒˆë¡œìš´ êµ¬ì¡° ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ì¸ë±ìŠ¤ë§Œ ìš”ì²­)
    prompt_structure_index_only = """
        The following text is part of a Thai legal document. Identify all hierarchical headers such as 'à¸«à¸¡à¸§à¸”', 'à¸ªà¹ˆà¸§à¸™', and 'à¸¡à¸²à¸•à¸£à¸²'.
        For each identified element, extract its type, title, and its character start/end positions within the provided text.
        Return the result as a JSON array.
        
        Example format:
        [
          { "type": "chapter", "title": "à¸«à¸¡à¸§à¸” 1", "start_index": 10, "end_index": 500 },
          { "type": "article", "title": "à¸¡à¸²à¸•à¸£à¸² 1", "start_index": 30, "end_index": 150 }
        ]
        
        [Text Chunk]
        {text_chunk}
    """

    for i, chunk in enumerate(document_chunks):
        status_container.write(f"3/{len(document_chunks)+2}: ì²­í¬ {i+1}/{len(document_chunks)}ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
        
        # LLM í˜¸ì¶œ
        response = model.generate_content(prompt_structure_index_only.format(text_chunk=chunk["text"]))
        headers_in_chunk = extract_json_from_response(response.text)
        
        if headers_in_chunk:
            for header in headers_in_chunk:
                # ë¡œì»¬ ì¸ë±ìŠ¤ë¥¼ ì „ì—­ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                header["global_start"] = header["start_index"] + chunk["global_start"]
                header["global_end"] = header["end_index"] + chunk["global_start"]
                all_headers.append(header)

    # 4. â­ï¸ ì¤‘ë³µ ì œê±° ë° ì •ë ¬ (ë‚˜ì¤‘ì— íŠ¸ë¦¬êµ¬ì¡° ë§Œë“¤ ë•Œ ì¤‘ìš”)
    status_container.write("ëª¨ë“  ì²­í¬ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤...")
    # global_startì™€ titleì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    unique_headers = list({ (h['global_start'], h['title']): h for h in all_headers }.values())
    # ì‹œì‘ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_headers = sorted(unique_headers, key=lambda x: x['global_start'])

    # 5. â­ï¸ ìµœì¢… ê²°ê³¼ ì¡°ë¦½ (ì´ì œ í…ìŠ¤íŠ¸ëŠ” ìµœì¢… ë‹¨ê³„ì—ì„œ ì¶”ê°€)
    # ê° í—¤ë”ì— ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ê°€í•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì—†ì´ Pythonìœ¼ë¡œ ì²˜ë¦¬)
    for header in sorted_headers:
        header["text"] = document_text[header["global_start"]:header["global_end"]]

    final_json = {
      "global_summary": generated_global_summary,
      "document_title": f"ë¶„ì„ëœ ë¬¸ì„œ (ëª¨ë¸: {model_name})",
      # ì•„ì§ í‰í‰í•œ ë¦¬ìŠ¤íŠ¸ì§€ë§Œ, ì´ì œëŠ” í›¨ì”¬ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
      "chapters": sorted_headers
    }
    
    return final_json