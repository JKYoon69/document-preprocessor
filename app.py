# app.py (ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ í…ŒìŠ¤íŠ¸ìš©)

import streamlit as st
import document_processor
import pandas as pd

st.set_page_config(page_title="ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ í…ŒìŠ¤íŠ¸", page_icon="ğŸ§ ")
st.title("ğŸ§  ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹(Semantic Chunking) ê²€ì¦")
st.write("ë‘ ê°€ì§€ ì²­í‚¹ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("ë²•ë¥  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", type=['txt'])

if uploaded_file is not None:
    document_text = uploaded_file.getvalue().decode('utf-8')
    st.write(f"íŒŒì¼ ì •ë³´: ì´ **{len(document_text):,}** ê¸€ì")

    if st.button("ë‘ ë°©ì‹ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ë¹„êµí•˜ê¸°"):
        
        # --- 1. ê¸°ì¡´ ë°©ì‹: ê¸€ì ìˆ˜ ê¸°ì¤€ ---
        st.subheader("1. ê¸€ì ìˆ˜ ê¸°ì¤€ (ë‹¨ìˆœ ë¶„í• )")
        char_chunks = document_processor.chunk_text_by_char(document_text)
        
        char_data = []
        for i, indices in enumerate(char_chunks):
            start, end = indices["start_char"], indices["end_char"]
            chunk_content = document_text[start:end]
            char_data.append({
                "ì²­í¬ ë²ˆí˜¸": i + 1,
                "ì‹œì‘ ìœ„ì¹˜": start,
                "ë ìœ„ì¹˜": end,
                "ê¸€ì ìˆ˜": len(chunk_content)
            })
        st.table(pd.DataFrame(char_data))

        # --- 2. ìƒˆë¡œìš´ ë°©ì‹: ì˜ë¯¸ ê¸°ë°˜ ---
        st.subheader("2. ì˜ë¯¸ ê¸°ë°˜ (Semantic Chunking)")
        semantic_chunks = document_processor.chunk_text_semantic(document_text)

        semantic_data = []
        for i, indices in enumerate(semantic_chunks):
            start, end = indices["start_char"], indices["end_char"]
            chunk_content = document_text[start:end]
            semantic_data.append({
                "ì²­í¬ ë²ˆí˜¸": i + 1,
                "ì‹œì‘ ìœ„ì¹˜": start,
                "ë ìœ„ì¹˜": end,
                "ê¸€ì ìˆ˜": len(chunk_content),
                "ë§ˆì§€ë§‰ 10ê¸€ì": "..." + chunk_content[-10:].replace("\n", "\\n")
            })
        st.table(pd.DataFrame(semantic_data))